{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1qNOIg5urxsQStF919k81_WYbFawT4m5o",
      "authorship_tag": "ABX9TyPwhyxW/Q0Tn2jC+SehW77e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacobyNevada/ds_belhard/blob/main/hw2/hw2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data_loader.py\n",
        "\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "class Data_Loader:\n",
        "    def data_loading(self, file_link, category, numeric_features, categorical_features):\n",
        "      try:\n",
        "        drive.mount('/content/drive')\n",
        "        file_path = f\"/content/drive/{file_link}\"\n",
        "\n",
        "        # Загрузка CSV файла\n",
        "        dataset = pd.read_csv(file_path)\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "          raise FileNotFoundError(f\"Файл {file_path} не найден\")\n",
        "\n",
        "        # Проверяем, что файл не пустой\n",
        "        if os.path.getsize(file_path) == 0:\n",
        "          raise ValueError(f\"Файл {file_path} пустой\")\n",
        "\n",
        "        return dataset[numeric_features + categorical_features + [category]]\n",
        "\n",
        "      except FileNotFoundError as e:\n",
        "        print(f\"Ошибка: {e}\")\n",
        "        print(\"Проверьте наличие файла в Google Drive\")\n",
        "        return None\n",
        "\n",
        "      except ValueError as e:\n",
        "        print(f\"Ошибка данных: {e}\")\n",
        "        return None\n",
        "\n",
        "      except Exception as e:\n",
        "        print(f\"Неожиданная ошибка: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "Jly1PuCGf9uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_procesor.py\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "class Data_Processor:\n",
        "\n",
        "  def data_processing(self, dataset, category, numeric_features, categorical_features):\n",
        "    X = dataset.drop(columns=[category])\n",
        "    y = dataset[category]\n",
        "\n",
        "    # Создание препроцессора\n",
        "    numeric_transformer = StandardScaler()\n",
        "    categorical_transformer = OneHotEncoder(drop='first')\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "      transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('cat', categorical_transformer, categorical_features)\n",
        "        ])\n",
        "\n",
        "    # Применение препроцессора к данным\n",
        "    X_processed = preprocessor.fit_transform(X)\n",
        "    return X_processed, y, preprocessor"
      ],
      "metadata": {
        "id": "ivmZwX7Bgco6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/JacobyNevada/ds_belhard.git\n",
        "\n",
        "# 2. Переходим в папку hw2\n",
        "%cd /content/ds_belhard/hw2\n",
        "\n",
        "# 3. Добавляем пути для импорта\n",
        "import sys\n",
        "sys.path.append('/content/ds_belhard')      # Корень репозитория\n",
        "sys.path.append('/content/ds_belhard/hw2')  # Папка hw2\n",
        "\n",
        "# 4. Проверяем какие файлы есть в папке hw2\n",
        "!ls -la /content/ds_belhard/hw2/"
      ],
      "metadata": {
        "id": "koX4CWKdnsVT",
        "outputId": "3bfedded-d802-47af-e18c-2566731dfd10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ds_belhard'...\n",
            "remote: Enumerating objects: 42, done.\u001b[K\n",
            "remote: Counting objects: 100% (42/42), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 42 (delta 13), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (42/42), 883.84 KiB | 6.55 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n",
            "/content/ds_belhard/hw2\n",
            "total 1144\n",
            "drwxr-xr-x 3 root root    4096 Sep 15 18:27 .\n",
            "drwxr-xr-x 6 root root    4096 Sep 15 18:24 ..\n",
            "-rw-r--r-- 1 root root    1671 Sep 15 18:24 data_procesor.py\n",
            "drwxr-xr-x 6 root root    4096 Sep 15 18:27 ds_belhard\n",
            "-rw-r--r-- 1 root root       1 Sep 15 18:24 .gitkeep\n",
            "-rw-r--r-- 1 root root 1144997 Sep 15 18:24 hw2.ipynb\n",
            "-rw-r--r-- 1 root root    1410 Sep 15 18:24 Readme.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "rkWnjzHUeI3V",
        "outputId": "f4160389-38a2-482c-9bcd-770ae63e9b21"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Data_Loader.data_loading() missing 1 required positional argument: 'categorical_features'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1719953128.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;31m# Загружаем информацию\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mData_Loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loading\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_link\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;31m# Показать всю информацию о датасете\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Data_Loader.data_loading() missing 1 required positional argument: 'categorical_features'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from data_loader import Data_Loader\n",
        "\n",
        "####################################\n",
        "# data_loader.py\n",
        "\n",
        "'''import pandas as pd\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "class Data_Loader:\n",
        "\n",
        "  def data_loading(file_link, category, numeric_features, categorical_features):\n",
        "    try:\n",
        "      drive.mount('/content/drive')\n",
        "      file_path = f\"/content/drive/{file_link}\"\n",
        "\n",
        "      # Загрузка CSV файла\n",
        "      dataset = pd.read_csv(file_path)\n",
        "\n",
        "      if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"Файл {file_path} не найден\")\n",
        "\n",
        "      # Проверяем, что файл не пустой\n",
        "      if os.path.getsize(file_path) == 0:\n",
        "        raise ValueError(f\"Файл {file_path} пустой\")\n",
        "\n",
        "      imputer = SimpleImputer(strategy='mean')\n",
        "\n",
        "      # Выбираем только числовые колонки для импутации\n",
        "      numeric_columns = numeric_features + [category]\n",
        "\n",
        "      # Применяем импутер\n",
        "      dataset[numeric_columns] = imputer.fit_transform(dataset[numeric_columns])\n",
        "\n",
        "      imputer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "      # Применяем к строковым колонкам\n",
        "      string_columns = categorical_features\n",
        "      dataset[string_columns] = imputer.fit_transform(dataset[string_columns])\n",
        "\n",
        "      return dataset[numeric_features + categorical_features + [category]]\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "      print(f\"Ошибка: {e}\")\n",
        "      print(\"Проверьте наличие файла в Google Drive\")\n",
        "      return None\n",
        "\n",
        "    except ValueError as e:\n",
        "      print(f\"Ошибка данных: {e}\")\n",
        "      return None\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"Неожиданная ошибка: {e}\")\n",
        "      return None'''\n",
        "\n",
        "\n",
        "'''def data_processing(dataset, category, numeric_features, categorical_features):\n",
        "  X = dataset.drop(columns=[category])\n",
        "  y = dataset[category]\n",
        "\n",
        "  # Создание препроцессора\n",
        "  numeric_transformer = StandardScaler()\n",
        "  categorical_transformer = OneHotEncoder(drop='first')\n",
        "\n",
        "  preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "          ('num', numeric_transformer, numeric_features),\n",
        "          ('cat', categorical_transformer, categorical_features)\n",
        "      ])\n",
        "\n",
        "  # Применение препроцессора к данным\n",
        "  X_processed = preprocessor.fit_transform(X)\n",
        "  return X_processed, y, preprocessor'''\n",
        "\n",
        "###############################\n",
        "# data_procesor.py\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "class Data_Processor:\n",
        "\n",
        "  def data_processing(dataset, category, numeric_features, categorical_features):\n",
        "    X = dataset.drop(columns=[category])\n",
        "    y = dataset[category]\n",
        "\n",
        "    # Создание препроцессора\n",
        "    numeric_transformer = StandardScaler()\n",
        "    categorical_transformer = OneHotEncoder(drop='first')\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "      transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('cat', categorical_transformer, categorical_features)\n",
        "        ])\n",
        "\n",
        "    # Применение препроцессора к данным\n",
        "    X_processed = preprocessor.fit_transform(X)\n",
        "    return X_processed, y, preprocessor\n",
        "\n",
        "###############################\n",
        "# data_trainer.py\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "class Data_Trainer:\n",
        "  def train_model(X, y):\n",
        "    model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
        "    model.fit(X, y)\n",
        "    return model\n",
        "\n",
        "\n",
        "  def predict(model, X):\n",
        "    return model.predict(X)\n",
        "\n",
        "\n",
        "  def evaluate_model(y_true, y_pred):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    report = classification_report(y_true, y_pred)\n",
        "    print(f\"Accuracy: {accuracy:.2f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(report)\n",
        "    #return accuracy, report\n",
        "\n",
        "\n",
        "###############################\n",
        "# data_visualiser.py\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Data_Visualiser:\n",
        "  def visualise_pairplot(dataset, category):\n",
        "    sample_data = dataset.head(50)\n",
        "    # Построение парных графиков при помощи sns\n",
        "    sns.pairplot(sample_data, hue=category)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "  def visualise_histogram(dataset):\n",
        "    # Установка стиля Seaborn для красивых графиков\n",
        "    sns.set(style=\"whitegrid\")\n",
        "\n",
        "    # Создание гистограмм для каждой числовой переменной\n",
        "    dataset.hist(bins=20, figsize=(15, 10), color='skyblue', edgecolor='black')\n",
        "\n",
        "    # Добавление названий для каждого графика и осей\n",
        "    for ax in plt.gcf().get_axes():\n",
        "      ax.set_xlabel('Значение')\n",
        "      ax.set_ylabel('Частота')\n",
        "      #ax.set_title(ax.get_title().replace('wine_class', 'Класс вина'))\n",
        "\n",
        "    # Регулировка макета для предотвращения наложения подписей\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Показать график\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "  def visualise_heatmap(dataset):\n",
        "    sns.set(style=\"white\")\n",
        "\n",
        "    # Расчет корреляционной матрицы только для числовых данных\n",
        "    numeric_df = dataset.select_dtypes(include=[np.number])  # Исключаем нечисловые столбцы\n",
        "    corr = numeric_df.corr()\n",
        "\n",
        "    # Маска для отображения только нижней треугольной части матрицы (опционально)\n",
        "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "\n",
        "    # Настройка цветовой палитры\n",
        "    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "\n",
        "    # Создание тепловой карты\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n",
        "\n",
        "    # Добавление заголовка\n",
        "    plt.title('Тепловая карта корреляций', fontsize=20)\n",
        "\n",
        "    # Показать график\n",
        "    plt.show()\n",
        "\n",
        "  def visualise_boxplot(dataset):\n",
        "    sns.set(style=\"whitegrid\")\n",
        "\n",
        "    # Создаем ящики с усами для каждой колонки в DataFrame\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    # Перебираем каждый числовой столбец и создаем для него ящик с усами\n",
        "    for index, column in enumerate(dataset.select_dtypes(include=[np.number]).columns):\n",
        "      plt.subplot((len(dataset.columns) // 3) + 1, 3, index + 1)\n",
        "      sns.boxplot(y=dataset[column])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "  def visualise_lineplot(dataset, numeric_features):\n",
        "    sns.set(style=\"whitegrid\")\n",
        "\n",
        "    # Создаем сетку графиков\n",
        "    fig, axes = plt.subplots(len(numeric_features), 1, figsize=(15, 20))\n",
        "\n",
        "    # Для случая с одним графиком (если нужен только один столбец)\n",
        "    if len(numeric_features) == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    datahead = dataset.head(20)\n",
        "\n",
        "    # Строим line plot для каждого числового столбца\n",
        "    for i, column in enumerate(numeric_features):\n",
        "        axes[i].plot(datahead.index, datahead[column], marker='o', linestyle='-',\n",
        "                    linewidth=2, markersize=3, alpha=0.7)\n",
        "        axes[i].set_xlabel('Индекс наблюдения')\n",
        "        axes[i].set_ylabel('Значение')\n",
        "        axes[i].set_title(f'Линейный график: {column}')\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "    # Регулировка макета для предотвращения наложения подписей\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Показать график\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "  def visualise_all_types(dataset, numeric_features, category):\n",
        "    visualise_pairplot(dataset, category)\n",
        "    visualise_histogram(dataset)\n",
        "    visualise_heatmap(dataset)\n",
        "    visualise_boxplot(dataset)\n",
        "    visualise_lineplot(dataset, numeric_features)\n",
        "\n",
        "###############################\n",
        "# data_informer.py\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "class Data_Informer:\n",
        "  def show_dataset(dataset):\n",
        "    print(f\"\\nDataset head:\\n{dataset}\\n\")\n",
        "\n",
        "\n",
        "  def show_info(dataset):\n",
        "    print(f\"\\nDataset info:\\n{dataset.info()}\\n\")\n",
        "\n",
        "\n",
        "  def show_describe(dataset):\n",
        "    print(f\"\\nDataset describe:\\n{dataset.describe()}\\n\")\n",
        "\n",
        "\n",
        "  def show_types(dataset):\n",
        "    print(f\"\\nDataset types:\\n{dataset.dtypes}\\n\")\n",
        "\n",
        "\n",
        "  def show_null_values(dataset):\n",
        "    print(f\"\\nDataset null values:\\n{dataset.isnull().sum()}\\n\")\n",
        "\n",
        "\n",
        "  def show_nan_values(dataset):\n",
        "    print(f\"\\nDataset NaN values:\\n{dataset.isna().sum()}\\n\")\n",
        "\n",
        "\n",
        "  def show_all_info(dataset):\n",
        "    show_dataset(dataset)\n",
        "    show_info(dataset)\n",
        "    show_describe(dataset)\n",
        "    show_types(dataset)\n",
        "    show_null_values(dataset)\n",
        "    show_nan_values(dataset)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################################\n",
        "\n",
        "# https://www.kaggle.com/datasets/nabihazahid/spotify-dataset-for-churn-analysis\n",
        "file_link = 'MyDrive/Colab Notebooks/spotify_churn_dataset.csv'\n",
        "\n",
        "# Определение числовых и категориальных признаков\n",
        "numeric_features = ['age', 'listening_time', 'songs_played_per_day', 'skip_rate', 'ads_listened_per_week', 'offline_listening']\n",
        "categorical_features = ['gender', 'country', 'device_type', 'subscription_type']\n",
        "category = 'is_churned'\n",
        "\n",
        "# Загружаем информацию\n",
        "dataset = Data_Loader.data_loading(file_link, category, numeric_features, categorical_features)\n",
        "\n",
        "# Показать всю информацию о датасете\n",
        "Data_Informer.show_all_info(dataset)\n",
        "\n",
        "# Обработка информации\n",
        "X, y, preprocessor = Data_Processor.data_processing(dataset, category, numeric_features, categorical_features)\n",
        "\n",
        "# Разделение данных на тренировочную и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Обучение модели\n",
        "model = Data_Trainer.train_model(X_train, y_train)\n",
        "\n",
        "# Предсказание на тестовых данных\n",
        "y_pred = Data_Trainer.predict(model, X_test)\n",
        "\n",
        "# Оценка модели\n",
        "Data_Trainer.evaluate_model(y_test, y_pred)\n",
        "\n",
        "# Показать все визуализации\n",
        "Data_Visualiser.visualise_all_types(dataset, numeric_features, category)"
      ]
    }
  ]
}